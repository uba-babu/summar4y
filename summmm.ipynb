{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80bf0052-df14-4c83-8423-dbc28cee43fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\u.b.a yadav\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\u.b.a yadav\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\u.b.a yadav\\anaconda3\\lib\\site-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "from transformers import pipeline\n",
    "!pip install PyPDF2\n",
    "!pip install --upgrade PyPDF2\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bc55367-22f5-4fa2-b493-715ece9834a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the PDF:\n",
      " Uchit Bhola Anju Babu is a full-stack AI engineer in Greater Noida, India . He is a data scientist with a strong foundation in statistical analysis, machine learning, and data visualization . He has a track record of driving impactful  outcomes .  At the center of the study is India's Indraprastha University, Delhi, India . The university has developed and deployed a YOLO -based vehicle deteering . The study was conducted by the Delhi-based institution .  YOLO model to accurately detect and classify vehicles such as cars, trucks, buses, and bikes . Implemented counting algorithms to track the number of vehicles and calculate their speeds in video . Integrated detection results into a live dashboard, enabling visualization and monitoring of traffic .  \"DeepQA\" developed a chatbot application utilizing deep learning techniques and integrated it with Flask framework . Enabled features like real -time order tracking, in -app payments, and notifications to enhance the user experience on mobile devices . Built emotion detection of tweets (happy, sad, neutral, surprise, surprise) using Tokenization, Stemming, and Lemmatization of sentences .  Predictive Model for Crop Recommendation System to recommend crops to be grown based on soil conditions and seasons in specific regions . Bot for writing essays and poems on different topics. bot for writing essay and poetry on  different topics.    predictive model for music recommendation system .  lt an attendance system using OpenCV, including face detection, face recognition, and maintaining attendance records . Interests : Artificial Intelligence, Renewable Energy, Music, Video Games, Basketball,  Video Games and   Artificial Intelligence .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Function to extract text from each page of the PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to summarize the extracted text in chunks to avoid long processing time\n",
    "def summarize_text(text, max_length=150, min_length=50, chunk_size=1000):\n",
    "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "    \n",
    "    # Break the text into chunks\n",
    "    text_chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    summary = \"\"\n",
    "    \n",
    "    # Summarize each chunk separately\n",
    "    for chunk in text_chunks:\n",
    "        chunk_summary = summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "        summary += chunk_summary[0]['summary_text'] + \" \"\n",
    "    \n",
    "    return summary.strip()\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = r\"C:\\Users\\U.B.A Yadav\\OneDrive\\Desktop\\Resume.pdf\"\n",
    "\n",
    "# Extract and summarize\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "summary = summarize_text(text)\n",
    "\n",
    "print(\"Summary of the PDF:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a88274db-911d-46d9-9866-94f6d991b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# JSON structure\n",
    "document_data = {\n",
    "    \"_id\": \"unique_resume_id\",  # This should be a unique identifier for each document\n",
    "    \"filename\": \"Resume.pdf\",\n",
    "    \"content\": text,  # Original content from the PDF\n",
    "    \"summary\": summary,  # Summarized content\n",
    "    \"keywords\": [\"resume\", \"skills\", \"experience\"],  # Optional: Extract keywords if needed\n",
    "    \"processed_at\": datetime.utcnow().isoformat()  # Timestamp\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbb21d52-86ba-49d6-a569-437a3e16bb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 125. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the PDF:\n",
      " The proposed method uses ambient sound input recorded in people’s homes to detect falls . We specifically target the bathroom environment as it is highly prone to falls and where existing techniques cannot be deployed without jeopardizing user privacy concerns . Our paper provides a novel non-wearable, non-intrusive, and scalable solu-tion for fall detection .  The solution is based on a Trans-former architecture that takes noisy sound input from bathrooms and classifies it into fall/no-fall class with an accuracy of 0.8673 . It is suitable for deploying in elderly homes, hospitals, and rehabilitation facilities without requiring the user to wear any device or be constantly \"watched\" by the sensors .  Fall is \"an event which results in a person coming to rest in-advertently on the ground or other lower-level\" Permission to make digital or hard copies of all or part of this work is granted without fee provided that copies are not made or distributed for profit or commercial advantage . Copyrights for components owned by others than ACM must be honored .  About 37.3 million falls require medical attention each year, and about 0.65 million result in deaths globally . The population aged 60-and older suffers the highest death rate due to a fall event . Non-fatal falls are equally devastating as they may lead to permanent disability and psychological fear of falling .  Different algorithms and algorithms exist to process the collected data to classify if a fall has occurred . These methods range from traditional statistical methods to more advanced machine learning [ 33] and deep learning [ 19], [30] based methods . The wearable and vision-based techniques have certain limitations .  The core of the proposed methodology is an ambient sound input that is analyzed to detect fall events . The use of sound input for fall detection offers an advantage over wearable and vision-based sensors . It does not re-programme a fall detection system for the elderly that is accurate, scalable and preserving user privacy .  Aims to detect falls in bathroom environments that are prone to fall events among the elderly . The method is less invasive than vision-based methods, and does not require the subject to be in sight . It is highly scalable as it is implemented on an autonomous mobile robot .  Current techniques have limitations regarding their deployment efficacy in bathroom environments . To overcome the need to wear a device and respect user privacy, we use sound input to solve the audio classification problem . The trained model is deployed on an autonomous mobile robot capable of navigating to the target (bathroom) environment and sending a phone text to an appropri-lyate party if needed .  A new set of features for audio classification, which we refer to as “Diff” features, show better performance than the traditional features such as log mel spectrograms and raw-audio features . The developed method is demon-ishlystrated for bathroom environments, it is extendable to all indoor environments with appropriate training data .  The techniques for fall detection are classified into two categories: wearable and non-wearable . The wearable techniques monitor human movement by constantly collecting accelerometer and gyro-scope data from devices the user wears . Such techniques commonly use threshold-based methods to distinguish falls from the normal human movement of a human subject .  wearable technology performs quite well, gaining more than 90% accuracy for detecting falls . The non-wearable techniques overcome these challenges as they are non-intrusive, and there is no concern of forgetfulness by the user . The more expensive non-sensors include vision/camera-based sensors that constantly stream video of the surroundings .  Recent vision-based solutions typically formulate fall detection as a human pose estimation and recognition problem as in [ 16], [22]. [22] proposes a system im-ishlyplemented on a mobile robot that uses its onboard RGB camera to identify falls . The underlying method is based on the You Only LookOnce (YOLOv3) deep learning model trained on the extended FallPerson Dataset . The system achieves a recall of 98.7% on the test set .  The most recent IoT-based solution is a custom-made device that measures vibrations and sound level where it is installed to detect falls . The method achieves 92.5% accuracy and no false negatives with 79.01% accuracy with some false negatives using an Artificial Neural Network (ANN)  There are few datasets for fall de-ggietection that are publicly available . The most recent approaches include Machine-Learning and Deep Learning-based methods . While these are easier to implement, they are susceptible to noisy data and generate more false alarms .  The publicly available fall datasets are predominantly from wearable devices and camera-based sensors . There are no publicly available audio fall datasets . Therefore, we self-collectively collected audio data for training and evaluating the proposed method . We carefully define scenarities and carefully define scenarios .  Participants mock all of the 8 scenarios shown in Table 1 to collect audio data from the bathroom environments of their own homes . The volunteers represent different genders, demography, and vary between 15 to 50 years old . All of the data is collected using smartphones. ios that purportedlyrepresent a realistic environment .  Data augmentation is a technique to increase the size of the dataset by applying various transformations to the original data . It helps the deep learning model generalize better, especially when the original dataset size is too small . We apply several transformations to fall and no-fall audio files . The specific transformations are outlined in Table 2 .  The Audiomentations1Python library is used to apply all audio augmentations . The augmented dataset is pre-processed and used to train and evaluate the proposed Transformer-based deep learning model . The tasks related to audio processing, such as reading raw audio files and feature extraction, are performed using librosa2Python .  The dataset is split into three subsets: train, validation, and test sets, with an 80:10:10 ratio . The train and validation sets are used to train and evaluate the model during the training phase . The test set is reserved for the final evaluation of the trained model .  The MFCC features are typically used for processing speech audios, whereas the log melphthalspectrograms are used for a broader class of audios . These features are considered hand-crafted features as one must define various parameters such as hop length, window type, and a number of melphthalbins to extract such features . This strategy affects the quality of the extracted features and hence the model output .  Person says key-propelled words demonstrating a true Fall such as ‘Help’, ‘Misty’ and ‘Call 911’ in a normal/soft/loud/normal tone . Person makes a very loud sound, 'Ahhhh’ or ‘Ouch’ as if hurt: Person says ‘I am playing with water’ Person says some words not relevant to a Fall, thus demonstrating a No Fall situation . Person humming softly: 2.3 Fall Running water (from shower or sink-toilet) and the person screams, demonstrating a Fall .  Person in the bathroom performing any activity such as cleaning bath--room/taking shower and singing or talking, etc. thus demonstrating a NoFall . Person makes a softer sound, ‘Ahhh’: 1-2-3 No Fall Running water (from shower or sink-or toilet) with no other sound inter-agogueference - just plain running water . Person banging on bathroom door demonstrating a true Fall situa-tion .  Person says ‘Help’, ‘Please come’ and ‘Could you help?’ multiple times and in a very loud tone: 6-7.7. 7.9.9 Fall Quiet bathroom environment with a person saying key words, demonstrat-ishlying a true Fall situation, such as 'Help', 'Misty'  Transformations can be applied at once . Apply various combinations of the transformations at once. Apply various filters individually . Add gaussian noise to the input. Apply time inversion or time shift shift. Apply tanh distortion to distort audio left/right in time-axis .  The deep learning model takes the raw input (in a sampled form), and it learns features during the training phase that it deems important for the task . The first feature type is a segmented version of the original sampled audio . Each sequence of t_seg seconds is then stacked vertically to create a 2-D input of size .  Figure 1 (b) shows an example of segmented audio derived from an original audio signal . The second type of feature explored in this paper derives from the segmented sampled audio described above . The idea behind creating this set of features is to capture the change between two consecutive audio sequences . Figure 1 shows that a small portion of samples (in this case, 560 discrete samples or 0.035 sec worth of audio) is dropped from the original audio .  A log mel spectrogram is attained by taking the Fourier transform (Fast Fourier Transform (FFT) of the original audio signal in the time-domain, then converting the resulting spectrogram to a Mel-scale . Several parameters, including the length of the STFT window, number of mel-bins, window type, and hop length (number of samples between consecutive samples) are used in the deep learning model .  Based on [ 21], [17] and experiments on ourIEEE/ACM CHASE ’22, November 17–19, 2022, Washington, D.C. Kaur, et al. created a log mel spectrogram of shape (88x64) for an audio signal that is 8.74sec long . This configuration results in a log-mel spectrogram (T) and M (M) of shape, where T is the number of frames and M is number of mel bins . These three features are fed independently into the deep learning model for further training and testing .  The model comprises of three main blocks: an input block, a feature-encoder block, and an output block . While the feature encoder block is the same regardless of the input type fed to the model, the input and output blocks vary slightly, yielding three different configurations .  The feature encoder block of our model is composed of sev-                eral encoder layers of a standard Transformer architecture [ 34]. Each Transformer encoder layer consists of Multi-Head Attention (MHA) and feed-forward sub-layers . In addition to the input received from the input layer of the model, we also receive a binary input mask .  During the training phase, the feature encoder block learns the embeddingfor the input . The MLP is a sequence of dense layers containing 265, 64, and 10-neurons . The output of the MLP feeds to the final classification layer, which consists of 2 neurons .  The size of the input fed to the model is (86x1600) The rest of the model architecture is the same as configuration A, described in Section 4.3.2.1.2 . The embedding of the CLS token is extracted from the feature encoder block and passed to the output block .  The model construction, training, and evaluation are configured using Keras API and Tensorflow 2.0 in a Google Colab environment . The trained model is saved on a local machine for inference at a later time . The hyper-parameters chosen for the training phase are perglytable 3.3.0 .  The intended application for fall detection is to create a robot guard for an indoor home environment . We deploy the trained model on a Misty II robot shown in Figure 3 . It is a mobile robot that is 0.35m in height, equipped with several sensors . If a potential “fall” is detected, the robot sends a text message to a registered phone number .  We use accuracy and F1-score as the metrics to assess our model . These metrics are commonly used in research within fall-classification domain [ 16], [9], [13], [12] Table 4 shows the overall results using different features and configurations discussed in the paper .  The ‘Diff’ features give the best results in terms of both accuracy and F1-score . The model can differentiate between most fall and no-fall categories . The instances where it struggles to give high recall are when the categories contain human speech . It implies that the model cannot distinguish between a normal human speech and distress calls .  The random forest and logistic regression yield anIEEE/ACM CHASE ’22, November 17–19, 2022, Washington, D.C. recision . In our future work, we will consider adding human speech recognition and emotion recognition from sound to be able to handle such challenging cases .  It is important to emphasize that the fall class in [ 8] only considers fall sounds that result from an impact on the floor when a person falls on the ground . The sounds representing no-falls contain war sounds and normal human conversations . The authors perform data acquisition using a special floor acoustic sensor to collect sounds of various objects falling .  The proposed method trains a SiameseNeural Network to generate embeddings used by a kNN classifier to classify a sound into fall or no-fall . The method achieves an F1-score of 93.58% . We believe that our method is at par with the state-of-the-art works if human speech data is excluded .  The number of encoder layers and the number of attention heads are two hy-progressiveperparameters we study first . Table 4: Results on the Test set comparing different features and methods . Table 5, 8, and 9 show the model performance for all three features .  Figure 4: Fall/No-Fall categories pairwise prediction results using Diff features and weighted loss . Ideally, a higher number of mel bins and shorter hop length should yield better results [ 21], which comes at a higher computational cost . Table 10 shows the model perform .  We choose 100ms as the segment length for segmentedraw audio and Diff features for the final model . Figure 5 shows the effect of the number of epochs on both training and validation sets in terms of the model accuracy . We selected 10 epochs as the hyperparameter for the training phase and validation phase of this experiment . The model achieves an accuracy of 0.9980.2% on the training set after 50 epochs .  Pepe (TxM) = 88x64cTransformer parameters: All other hyperpa-privat-phthalrameters used are per Table 3 . Table 6: Results of different Mel bins for grotesquelog mel spectrograms on the Test set . Table 7: Results . of different Hop length for log mel spectro-phthalgrams .  Heads Layers Accuracy F1-score: Peers, Peers and Peers Layers (N-1 x D) used are per Table 3 . Peers were used to measure the length of a segment using Diff features . Peer Layers were used in Table 9: Results of different heads and.encoder layers, using Diff . features an.egegregegegebTransformer (NxD) = (87x1600)  After 50 epochs, F1-score is 0.7620, with an F1 score of 0.4091 . Table 11 shows that the combined features do not yield any better performance than using the Diff features alone . Table 10: Results of different heads and encoder layers, using log mel.spectrograms and raw features com-ensiblybined .  Researchers develop a Transformer-based deep learning model to detect fall events from the sound input . The proposed solution is extensible to indoor environments beyond the bathroom with additional training . To further improve upon the results presented in the paper, we plan to collect more variety of fall events .  Deep Learning:Approaches for Fall Detection Using Acoustic Information . In Advances in SmartGrid Technology . Springer, Springer, 479–488. 2025. 2021. 2030. 2020. The study was published on the arXiv.com/ArXiv: 2104.00769(2021)  The Bluetooth-enabled Bluetooth-based Fall Detection and Warning System for Elderly People in the Journal of Sensors 2022 (2022) Researchers are developing a low-energy-based fall detection system for elderly people in nursing homes . Researchers are also developing a wearable sensor to detect falls on elderly people .  Researchers: Pre-training of deep bidirectional transformers for language understanding . They say: \"Pre-training is a tool for language-recognition for the first time\" Researchers: \"Few-shot siamese neural networks employing audio-features for human-fall detection\"  Researchers: Fall detection for elderly in assisted environments: Video surveillance systems and challenges . Researchers: An Android application for Use with a Wearable Accelerometer Patch for Fall Detection will be developed in 2021 . The research was conducted by Shabnam Ezatzadeh and Mohammad Reza Keyvanpour .  Researchers: Large-scale pretrained audio neural networks for audio-recognition . In 2021 IEE will publish a survey of IoT-based fall detection for the elderly . Researchers: Fall detection system embedded on a mobile platform. 2022. RGB camera-based fallen person detection system .  A descriptive analysis of location of older adult falls that resulted in emergency department visits in the United States, 2015 . The study was published in the journal of the American journal of lifestyle medicine 15, 6 (2021), 590–597. The fall rate of older adults in the U.S. increased in the community-dwelling older adults .  Anju S Pillai, Sejal Badgujar, and Sujatha Krishnamoorthy. 2020. Prevalence of falls and its.characteristics among malaysian older adults: a review. Medicine & Health 15, 1 (2020)  The study of fall detection and prevention using machine-learning is published by Lode Vuegen. and the journal Sensors. Sensors 17, 1 (2017), 198 . The study was published in 2017, and is published in 2018, with a number of peer-peer peer-reviewed articles .  HONEY: a multimodality fall-detection and telecare system. 2014. ol 72 (2022), 103355. 2014 . 2015. A survey on vision-based fall detection. In Proceedings of the 8th ACM international conference on PErvasive technologies related to assistive environments .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Set environment variable to suppress the symlink warning\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "from transformers import pipeline\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Function to extract text from each page of the PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    pdf_reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to summarize the extracted text in chunks to avoid long processing time\n",
    "def summarize_text(text, max_length=150, min_length=50, chunk_size=1000):\n",
    "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "    \n",
    "    # Break the text into chunks\n",
    "    text_chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    summary = \"\"\n",
    "    \n",
    "    # Summarize each chunk separately\n",
    "    for chunk in text_chunks:\n",
    "        chunk_summary = summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "        summary += chunk_summary[0]['summary_text'] + \" \"\n",
    "    \n",
    "    return summary.strip()\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = r\"C:\\Users\\U.B.A Yadav\\OneDrive\\Desktop\\frequency.pdf\"\n",
    "\n",
    "# Extract and summarize\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "summary = summarize_text(text)\n",
    "\n",
    "print(\"Summary of the PDF:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54347b6-977f-40aa-9748-862ca9188609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
